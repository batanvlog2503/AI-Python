{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "204fa0c7-161a-4775-abe6-ebeca127bd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hi·ªÉu ƒë∆°n gi·∫£n tr∆∞·ªõc khi ƒë∆∞a v√†o m√¥ h√¨nh h·ªçc m√°y ƒë·ªÉ chuyern th√†nh vector\n",
    "# ta ph·∫£i ti·ªÅn x·ª≠ l√≠, l√†m s·∫°ch n√≥ tr∆∞·ªõc khi ƒë∆∞a v√†o\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534acdfb-e582-40f9-a905-f0bbb681c767",
   "metadata": {},
   "source": [
    "# Working with Text Data - Text Preprocessing\n",
    "## Text Preprocessing steps\n",
    "Text Preprocessing steps include some essential tasks to clean and remove the noise from the available data\n",
    "1. Removing special Characters and Punctuation\n",
    "2. Converting to LowerCase\n",
    "3. Tokenization( Sentence Tokenization and Word Tokenization)\n",
    "4. removing stop Words\n",
    "5. stemming or lemmatization\n",
    "6. HTML parsing and Cleanup\n",
    "7. Spell Correction\n",
    "8. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ee9846-31d7-4a4b-bece-c5cf9d4e7898",
   "metadata": {},
   "source": [
    "## 1. Removing Special Characters and Punctuation\n",
    "Special characters like ^, ~, @, $, etc... Punctuations like ., ?, ,, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84dfaf94-bdc8-451a-8d87-25fa99556798",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "We're LaRninG1 Natural-LAnguage-Processing!üòÄ üöÄ ‚ù§Ô∏è \n",
      "In this\\ example wE are goIng to Learn variouS text9 preprocessing steps.\n",
      "I'm GoIng TO-bE Mr. Rich. ‚Çπ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_36588\\2423723600.py:1: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  raw_text = \"\"\"\n"
     ]
    }
   ],
   "source": [
    "raw_text = \"\"\"\n",
    "We're LaRninG1 Natural-LAnguage-Processing!üòÄ üöÄ ‚ù§Ô∏è \n",
    "In this\\ example wE are goIng to Learn variouS text9 preprocessing steps.\n",
    "I'm GoIng TO-bE Mr. Rich. ‚Çπ \n",
    "\"\"\"\n",
    "\n",
    "print(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5d339763-f8c2-4f8b-bd95-819afb952638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ceba432-9e51-49dd-bf71-18c447e87719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Were LaRninG NaturalLAnguageProcessingüòÄ üöÄ ‚ù§Ô∏è \n",
      "In this example wE are goIng to Learn variouS text preprocessing steps\n",
      "Im GoIng TObE Mr Rich ‚Çπ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# let'ss now use 'string' module to clear up all the punctuations\n",
    "text = \"\".join([char for char in raw_text if char not in string.punctuation and not char.isdigit()])\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01853291-04fb-480a-9667-56918bf75d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toi ƒë√£ lo·∫°i b·ªè k√≠ t·ª± ƒë·∫∑c bi·ªát v√† ch·ªØ s·ªë"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d931e3a-9f47-4995-ab09-96c035cbafed",
   "metadata": {},
   "source": [
    "## A more powerfull weapon to remove special characters and punctuation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2c098da2-1c39-48f8-a47b-8e285d853579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " We re LaRninG  Natural LAnguage Processing!        In this  example wE are goIng to Learn variouS text  preprocessing steps. I m GoIng TO bE Mr. Rich.    \n"
     ]
    }
   ],
   "source": [
    "import re # regex\n",
    "\n",
    "# lets define a regex to match special charaters and ƒëigitss\n",
    "\n",
    "regex = \"[^a-zA-Z.!]\"\n",
    "# a-zA-Z l√† l·∫•y t·∫•t c·∫£ ch·ªØ c√°i\n",
    "# .! l√† l·∫•y c·∫£ d·∫•u ch·∫•m v√† d·∫•u than\n",
    "# ^ l√† ph·ªß ƒë·ªãnh nghƒ©a l√† kh√¥ng ph·∫£i nh·ªØng t·ª´ trong ƒë√≥ \n",
    "\n",
    "\n",
    "text = re.sub(regex, \" \", raw_text)\n",
    "print(text) # nghƒ©a l√† thay th·∫ø nh·ªØng t·ª´ c·ªßa raw_text kh√¥ng c√≥ trong regex = \" \";"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a7772f-915c-4f7a-a02c-f9f5d5fccaea",
   "metadata": {},
   "source": [
    "## 2. Converting to LowerCase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3133e630-af7b-4428-b66c-953669b9bad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we re larning  natural language processing!        in this  example we are going to learn various text  preprocessing steps. i m going to be mr. rich.    \n"
     ]
    }
   ],
   "source": [
    "text = text.lower()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b37f422-bc3f-477a-84b0-758985bd4c02",
   "metadata": {},
   "source": [
    "## 3. Tokenization( Sentence Tokenization and Word Tokenization)\n",
    "## nghƒ©a l√† t√°ch th√†nh c√¢u ho·∫∑c t·ª´\n",
    "this is a simple step to break the text into sentences and words \n",
    "\n",
    "note that in, any NLP task tokenization is one of the most important step. Hence, any NLP pipeline has to start with a reliable system to split the text into sentences and furrther split a sentence into words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80a0ca2b-4237-489c-90ba-9ef1269dbba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " we re larning  natural language processing!        in this  example we are going to learn various text  preprocessing steps. i m going to be mr. rich.    \n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7efd138a-f18c-4de1-9bf6-bb7158b9de5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'larning', 'natural', 'language', 'processing!', 'in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps.', 'i', 'm', 'going', 'to', 'be', 'mr.', 'rich.']\n"
     ]
    }
   ],
   "source": [
    "words = text.split();\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42c32691-884a-450a-ac8a-c6c144b56533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we re larning  natural language processing!        in this  example we are going to learn various text  preprocessing steps', ' i m going to be mr', ' rich', '    ']\n"
     ]
    }
   ],
   "source": [
    "sentences = text.split(\".\")\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d3b9f04-90ad-44f5-a2dc-514c53d6e092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in c:\\users\\admin\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5f120cf5-5088-496d-9c2c-52962e47061e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Natural language toolkit th∆∞ vi·ªán x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n\n",
    "# ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n",
    "# l√†m vi·ªác v·ªõi corpora\n",
    "# x√¢y d·ª±ng c√°c ·ª©ng d·ª•ng NLP c∆° b·∫£n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6c5f0b1b-4b20-45b8-a6ae-d3f492c52b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nghƒ©a l√† t·ª´ n√£y t·ªõi gi·ªù ta l√†m theo ki·ªÉu th·ªß c√¥ng\n",
    "# c√≤n gi·ªù ta s·∫Ω d·ª´ng th∆∞ vi·ªán v·ªõi nhi·ªÅu ch·ª©c nƒÉng"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91ed163-970c-4973-89f4-46cc99f70225",
   "metadata": {},
   "source": [
    "## Sentence tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3138ac62-0f3f-48b6-964e-9a5c315644fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download the Punkt tokenizer models from the nltk\n",
    "# The Punkt tokenizer is used for sentence and word tokenization\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c51b3a0-5589-4bee-b8ef-463884fda9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' we re larning  natural language processing!', 'in this  example we are going to learn various text  preprocessing steps.', 'i m going to be mr. rich.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "# t√°ch vƒÉn b·∫£n th√†nh c√°c c√¢u sent_tokenize\n",
    "# tokenize text into sentences\n",
    "my_sentences = sent_tokenize(text)\n",
    "print(my_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3556ed3-bc61-43c5-8c1a-20c009e90438",
   "metadata": {},
   "source": [
    "## Word tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ac0f549-bced-49e1-a38d-9b61195aa0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'larning', 'natural', 'language', 'processing', '!', 'in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'i', 'm', 'going', 'to', 'be', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# tokenize text to words\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1333ccc2-777b-4110-83c5-fe6f2a9a19b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'larning', 'natural', 'language', 'processing', '!']\n",
      "['in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.']\n",
      "['i', 'm', 'going', 'to', 'be', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# tokenize sentence to words\n",
    "for word in my_sentences: # x√©t c√°c h√†ng\n",
    "    print(word_tokenize(word))# t√°ch c√°c h√†ng th√†nh ch·ªØ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8853ffd-702e-41ed-b1b8-7351f5f4e926",
   "metadata": {},
   "source": [
    "## 4. Removing Stop Words\n",
    "\n",
    "stopwords. don't contribute to the meaning of a sentence. So, wer can safely remove them without changing the meaning of the sentecnce \n",
    "EXAMple a, an, the, was, is, by are the stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8f8d5ec7-2877-431a-aba9-7369544dc3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db9c71f7-97af-43b5-be31-5a2d76881e26",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download the stopwords corpus\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "166f1e25-1f13-4506-8354-f47ddeb9bf02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of stopwords:\n",
      "['a', 'about', 'above', 'after', 'again', 'against', 'ain', 'all', 'am', 'an', 'and', 'any', 'are', 'aren', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'couldn', \"couldn't\", 'd', 'did', 'didn', \"didn't\", 'do', 'does', 'doesn', \"doesn't\", 'doing', 'don', \"don't\", 'down', 'during', 'each', 'few', 'for', 'from', 'further', 'had', 'hadn', \"hadn't\", 'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'having', 'he', \"he'd\", \"he'll\", 'her', 'here', 'hers', 'herself', \"he's\", 'him', 'himself', 'his', 'how', 'i', \"i'd\", 'if', \"i'll\", \"i'm\", 'in', 'into', 'is', 'isn', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', \"i've\", 'just', 'll', 'm', 'ma', 'me', 'mightn', \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'my', 'myself', 'needn', \"needn't\", 'no', 'nor', 'not', 'now', 'o', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'our', 'ours', 'ourselves', 'out', 'over', 'own', 're', 's', 'same', 'shan', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\", 'should', 'shouldn', \"shouldn't\", \"should've\", 'so', 'some', 'such', 't', 'than', 'that', \"that'll\", 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'there', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 've', 'very', 'was', 'wasn', \"wasn't\", 'we', \"we'd\", \"we'll\", \"we're\", 'were', 'weren', \"weren't\", \"we've\", 'what', 'when', 'where', 'which', 'while', 'who', 'whom', 'why', 'will', 'with', 'won', \"won't\", 'wouldn', \"wouldn't\", 'y', 'you', \"you'd\", \"you'll\", 'your', \"you're\", 'yours', 'yourself', 'yourselves', \"you've\"]\n"
     ]
    }
   ],
   "source": [
    "print(\"List of stopwords:\")\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "efaf2f5c-f2fe-43f2-b5be-f04c12faea21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'larning', 'natural', 'language', 'processing', '!', 'in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'i', 'm', 'going', 'to', 'be', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "df89f2b8-f04b-4d58-bc59-e94fa93e3bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larning', 'natural', 'language', 'processing', '!', 'example', 'going', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'going', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "# removing stop words in stopwords.words(\"English\")\n",
    "words = [word for word in words if word not in stopwords.words(\"english\")]\n",
    "\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f5e595-ad22-4492-b4f0-85767875bf37",
   "metadata": {},
   "source": [
    "## 5 Stemming hi·ªÉu ƒë∆°n gi·∫£n l√† c·∫Øt bro h·∫≠u t·ªï ƒë∆∞a v·ªÅ g·ªëc t·ª´ \n",
    "## c√≥ th·ªÉ kh√¥ng ƒë√∫ng ng·ªØ ph√°p v√≠ d·ª• studies, stud\n",
    "ƒë∆∞a t·ª´ v·ªÅ d·∫°ng chu·∫©n trong t·ª´ ƒëi·ªÉn (ng·ªØ ph√°p ch√≠nh x√°c h∆°n).\n",
    "\n",
    "\"studies\", \"studying\" ‚Üí \"study\"\n",
    "\n",
    "M·ª•c ƒë√≠ch: gom c√°c bi·∫øn th·ªÉ c·ªßa m·ªôt t·ª´ v·ªÅ c√πng m·ªôt d·∫°ng."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1022384-9468-4d76-bef6-b1ad17836484",
   "metadata": {},
   "source": [
    "Stemming is the process of removing suffiexs and reducing a word to some root from such that alll different variant of that word can be represented by the same form. \n",
    "-  warm, warmer, warming can be converted to warm\n",
    "This is accomplished by applying a fixed set of rules ( if the words ends with \"es\")\n",
    "\n",
    "Stemming is commonly used in text classification to reduce the feature space to train machine learning models.\n",
    "\n",
    "Popular stemming techniques are:\n",
    "\n",
    "Porter Stemmer\n",
    "Snowball Stemmer (AKA Porter2)\n",
    "Lancaster Stemmer (Fastest approach, but not advised to use.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "24a02028-1cfc-4c93-baae-d13811dfaf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larning', 'natural', 'language', 'processing', '!', 'example', 'going', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'going', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "10008de1-b855-4820-acd0-d14006671bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['we', 're', 'larning', 'natural', 'language', 'processing', '!', 'in', 'this', 'example', 'we', 'are', 'going', 'to', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'i', 'm', 'going', 'to', 'be', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0dc879a4-bf99-4874-aa91-aa87c84d6a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larn', 'natur', 'languag', 'process', '!', 'exampl', 'go', 'learn', 'variou', 'text', 'preprocess', 'step', '.', 'go', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "# stemming\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "## initial the inbuilt Stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "clean_tokens_stem = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(clean_tokens_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74b3fe9-8a58-4b36-9dc8-b24f53fbf73f",
   "metadata": {},
   "source": [
    "## 6. Lemmatization\n",
    "Lemmatization is the process of mapping all the different forms of a word to its base word. While this seems cloes to the definition , they are in fact, different. Lemmatization uses more linguistic knowledge to keep output human readable\n",
    "\n",
    "- ƒê∆∞a c√°c bi·ªÉn th·ªÉ v·ªÅ 1 t·ª´ g·ªëc chu·∫©n t·ª≠ trong t·ª´ ƒëi·ªÉn (lenma)\n",
    "- n√≥ tr·∫£ v·ªÅ ki·∫øn th·ª©c ng√¥n ng·ªØ h·ªçc n√™n k·∫øt qu·∫£ l√† t·ª´ th·∫≠t\n",
    "- v√≠ d·ª• studies, studying. studied-> study\n",
    "- better->good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c1d6492d-74b9-4c7b-a6ec-519147e79ad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Downloading wordnet before applying Lemmatizer\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94e405b4-16b2-4d57-bd62-aa7c647331a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larning', 'natural', 'language', 'processing', '!', 'example', 'going', 'learn', 'various', 'text', 'preprocessing', 'step', '.', 'going', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "# lemmatizing\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## we can also use lemmatier instead of Stemmer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "clean_tokens_lem = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(clean_tokens_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab0d11aa-2ad5-4070-ae5c-23a42b009546",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['larning', 'natural', 'language', 'processing', '!', 'example', 'going', 'learn', 'various', 'text', 'preprocessing', 'steps', '.', 'going', 'mr.', 'rich', '.']\n"
     ]
    }
   ],
   "source": [
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "37ae3421-13a2-4cb5-8ed5-24bfe9d81639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v√≠ d·ª• steps trong words ƒë√£ th√†nh step trong clean_tokens_lem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fe2cdc4-905c-4b92-bfa0-5e42c05f5159",
   "metadata": {},
   "source": [
    "## putting all the steps together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f6796eec-4baf-439e-88f4-be4cd1bf26ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are Learning Machine Learning $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing natural - language data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we Are Mimicing natural intelligence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text\n",
       "0    We are Learning Machine Learning $\n",
       "1   Processing natural - language data.\n",
       "2     10 machine - learning algorithms.\n",
       "3  we Are Mimicing natural intelligence"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lst_text = [\"We are Learning Machine Learning $\", \n",
    "            \"Processing natural - language data.\", \n",
    "            \"10 machine - learning algorithms.\", \n",
    "            \"we Are Mimicing natural intelligence\"]\n",
    "\n",
    "df = pd.DataFrame({'text': lst_text})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3feae959-cdec-472d-bc41-87b2ca8b4ae3",
   "metadata": {},
   "source": [
    "## Explicitly cleaning the Text and Vectorizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e37f0fae-d2e0-4842-9d3c-f5267164cab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize  import sent_tokenize\n",
    "\n",
    "\n",
    "# create function to clean document\n",
    "def clean(doc): # v·ªõi doc l√† t√†i li·ªáu\n",
    "    # doc is a string of text\n",
    "    # lets' define a regex to match special characters and digits\n",
    "\n",
    "    regex = \"[^a-zA-Z.]\"\n",
    "    doc = re.sub(regex, \" \", doc)\n",
    "\n",
    "    #convert to lowercase\n",
    "    doc = doc.lower()\n",
    "\n",
    "    #tokenization\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "\n",
    "    #stopwords removal\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    #Lemmatize\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in filtered_tokens]\n",
    "    # join and return \n",
    "    return  \" \".join(lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "360aafea-2785-4820-a3af-efd7ca8ffc33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are Learning Machine Learning $</td>\n",
       "      <td>learning machine learning</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing natural - language data.</td>\n",
       "      <td>processing natural language data .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms.</td>\n",
       "      <td>machine learning algorithm .</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we Are Mimicing natural intelligence</td>\n",
       "      <td>mimicing natural intelligence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text                          clean_text\n",
       "0    We are Learning Machine Learning $           learning machine learning\n",
       "1   Processing natural - language data.  processing natural language data .\n",
       "2     10 machine - learning algorithms.        machine learning algorithm .\n",
       "3  we Are Mimicing natural intelligence       mimicing natural intelligence"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create column after clean text\n",
    "\n",
    "df['clean_text'] = df['text'].apply(lambda x : clean(x))\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975e62e0-06bd-4636-901a-bdd1969c20bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
