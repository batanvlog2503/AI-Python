{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b20647bd-7a75-41c0-83b7-4ed5adc3a23a",
   "metadata": {},
   "source": [
    "# Working With Text Data - Text Transformation ( AKA Vectorization)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe04a4-286c-4d07-a06c-6717d36395eb",
   "metadata": {},
   "source": [
    "## Text Data\n",
    "\n",
    "Text Analysis is a major application field for machine learning algorithms. Some of the major application areas of NLP are:\n",
    "1. Spell Checker, Keyword Search\n",
    "2. Sentiment Analysis, Spam Classification\n",
    "3. Machine Translation\n",
    "4. Chatbots / Dialog Systems\n",
    "5. Question Answering Systems\n",
    "\n",
    "However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "## Why NLP is hard ?\n",
    "1. Complexity of representation\n",
    "2. Ambiguity in Natural Language (sự mơ hồ trong NLP)\n",
    "\n",
    "## Vectorization Techniques(AKA feature Engineering or Extraction - Convert text to Numerical Vectors)\n",
    "\n",
    "1. Bag of words ( Phương pháp đơn giản nhất, chỉ đếm số lần mỗi từ xuất hiện trong một văn bản. Nó bỏ qua ngữ pháp và thứ tự từ. Vector kết quả sẽ có kích thước bằng từ điển và mỗi giá trị là tần số của một từ)\n",
    "2. TF IDF( Term Frequency -Inverse Document Frequency  Một cải tiến của BoW. Nó không chỉ đếm tần suất từ mà còn đánh trọng số cho từ đó. Những từ xuất hiện nhiều trong một văn bản nhưng hiếm trong toàn bộ tập dữ liệu sẽ có trọng số cao hơn)\n",
    "3. Word2Vec (by google  Được tạo ra để dự đoán ngữ cảnh xung quanh một từ (Skip-gram) hoặc dự đoán một từ dựa trên ngữ cảnh của nó (CBOW). Các từ có ngữ nghĩa tương tự sẽ có vector gần nhau trong không gian vecto)\n",
    "4. Glove(Global Vectors by Standford Dựa trên ma trận đồng xuất hiện toàn cục của các từ. GloVe kết hợp cả ngữ cảnh cục bộ (như Word2Vec) và thống kê toàn cục của tập dữ liệu.)\n",
    "5. Fast Text (Mở rộng Word2Vec bằng cách xem xét các ký tự con (subword). Điều này giúp FastText xử lý tốt các từ không có trong từ điển (out-of-vocabulary words) và các ngôn ngữ có cấu trúc phong phú.)\n",
    "6. ELMo ( Embeddings from Language Models Sử dụng mạng nơ-ron hồi quy hai chiều (bi-directional LSTMs) để tạo ra các nhúng từ ngữ cảnh. Ví dụ, từ \"bank\" trong \"river bank\" sẽ có vector khác với \"bank\" trong \"savings bank\".)\n",
    "7. GPT( Generative Pre_Trained Transformer by OpenAI)\n",
    "8. BERT( Bidirectional Encoder representations From Transformer by Google  Một mô hình đột phá sử dụng kiến trúc Transformer. BERT được huấn luyện để hiểu ngữ cảnh hai chiều (cả trước và sau) của một từ. Đây là lý do nó hiệu quả hơn các mô hình chỉ nhìn ngữ cảnh một chiều.)\n",
    "9. LLM ( Large Language Model  Các LLMs như BERT và GPT không chỉ là công cụ để vector hóa; chúng còn có khả năng thực hiện nhiều tác vụ NLP phức tạp khác (ví dụ: tóm tắt, dịch thuật, trả lời câu hỏi) nhờ vào khả năng nắm bắt ngữ nghĩa và ngữ cảnh sâu sắc của chúng. Khi chúng ta sử dụng LLM để vector hóa một đoạn văn bản, chúng ta đang lấy các \"state\" hoặc \"embeddings\" nội tại của mô hình, mà các vector này đã được mã hóa với ngữ cảnh phong phú của câu.)\n",
    "10. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c850f8d7-5311-4607-8e2b-9926a6eea668",
   "metadata": {},
   "source": [
    "## Only the Following Techniques are covered in this notebook\n",
    "\n",
    "1. Bag of words\n",
    "2. TF IDF(Term Frequency - Inverse Document Frequency)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9d1193-e30e-4bce-9bd6-078e14f8054c",
   "metadata": {},
   "source": [
    "### We use CountVectorizer to convert text into a matrix of token count\n",
    "### We are going to perform below mentioned steps to understand the entire process:\n",
    "1. Convertign text to numerical vectors with the help of CountVectorizer\n",
    "2. Understand fit and transformer\n",
    "3. looking at Vocabulary\n",
    "4. Converting space matrix to dense matrix using toarray()\n",
    "5. Understaning n_gram\n",
    "## Advantage\n",
    "1. It is simple to understand and implement like OneHotCoding\n",
    "2. We have a fixe length encoding for any sequence of arbitrary length\n",
    "3. Documents with same words /  vocabulary will have similar representation . So if two documents have a similars vocabular, they'll be closer to each other in this vector and vice versa#\n",
    "## Disvantage\n",
    "1. The size (big)\n",
    "2. hiểu đơn giản walk, walked, walking là 3 từ nghĩa giống nhau nhưng nó sẽ hiểu là 3 từ khác nhau\n",
    "3. từ ngoài từ điển (out of vocabulary)\n",
    "4. overfitting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdd0512-dad6-49c0-a7bc-79ede98bdbea",
   "metadata": {},
   "source": [
    "#  a. BoW Text Vectorization: Apply CountVectorizer\n",
    "\n",
    "1. encoding : str, default  = 'utf-8' ( xác định bộ mã hóa kí tự)\n",
    "2. decode_error:{'strict',, 'ignore', 'replace'}, default = 'strict'\n",
    "- xử lý trường hợp không hợp lệ với encoding\n",
    "- strict : báo lỗi\n",
    "- ignore : bỏ qua kí tự đó\n",
    "- replace là thanh thế bằng kí tự đặc biệt thường là ?\n",
    "\n",
    "3. token_pattern: str or None, default=r\"(?u)\\b\\w\\w+\\b\"\n",
    "- dùng để tách các từ ký tự\n",
    "- default chọn các từ  >= 2 ký tự , tự bỏ dấu câu\n",
    "\n",
    "4. tokenizer: callable , default = None\n",
    "- Cho phép tùy chỉnh hàm token\n",
    "- Nếu đặt None, dùng Regex mặc định hoặc token_pattern\n",
    "\n",
    "5. ngram_range: tuple(min_n,max_n), default = (1, 1)\n",
    "- xác định n - gram sẽ trich xuâts\n",
    "- (1, 1) chỉ unigram 1 từ\n",
    "- (1, 2) unigram + bigram ( 2 từ)\n",
    "\n",
    "6. strip_accents: {'ascii', 'unicode'}, default = None\n",
    "- loại bỏ dấu và chuararn hóa kí tự\n",
    "- ví dụ é thành e\n",
    "- unicode chuẩn hóa unicode\n",
    "- thống nhất văn bản trước khi tách token\n",
    "\n",
    "7. lowercase: bool, default = True\n",
    "- Chuyển thành ký tự chữ thường trước khi token\n",
    "\n",
    "8. preprocessor: callable, default = None\n",
    "- Cho phép tùy chỉnh bước tiền xử lý (trức tokenization)\n",
    "\n",
    "9. stop_words:{'english', alisst, default = None}\n",
    "- loại bỏ các từ dừng : từ thong dụng, khong mang ý nghĩa đặc trưng\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0e07ca-8562-405e-9c5c-d4bbaaafe052",
   "metadata": {},
   "source": [
    "# Import the CountVectorizer i.e Bag of Words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c858842-84f9-485c-afc9-2b613f0a3ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (3, 8)\n",
      "Type of Numerical Representation: <class 'scipy.sparse._csr.csr_matrix'>\n",
      "Vocabulary learned: ['123' 'and' 'is' 'numbers' 'symbols' 'test' 'this' 'with']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"This is a test\", \"with numbers 123\", \"and symbols #!\"]\n",
    "\n",
    "# initialize the object\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# fit and transform\n",
    "\n",
    "num_rep = vectorizer.fit_transform(text)\n",
    "\n",
    "print(\"Shape:\", num_rep.shape)\n",
    "print(\"Type of Numerical Representation:\", type(num_rep))\n",
    "print(\"Vocabulary learned:\", vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0da21-20ce-4272-9112-9951d4fd565e",
   "metadata": {},
   "source": [
    "### Important Observation\n",
    "1. By Default CountVectorizer took care of removing special characters\n",
    "2. It only reads alphanumeric characters and tokenize only if the length is greater than or equal to 2 ( >= 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72bf3f01-1afd-43f5-9f77-2fe023704d4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We are Learning Machine Learning $</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Processing natural - language data.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10 machine - learning algorithms.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>we Are Mimicing natural intelligence</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   text\n",
       "0    We are Learning Machine Learning $\n",
       "1   Processing natural - language data.\n",
       "2     10 machine - learning algorithms.\n",
       "3  we Are Mimicing natural intelligence"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "lst_text = [\"We are Learning Machine Learning $\", \n",
    "            \"Processing natural - language data.\", \n",
    "            \"10 machine - learning algorithms.\", \n",
    "            \"we Are Mimicing natural intelligence\"]\n",
    "\n",
    "df = pd.DataFrame({'text': lst_text})\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2147615a-e18c-45eb-bba7-6d1cc8e1ad8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# in the next Sectio : Custom Text Cleaning\n",
    "# We will study a problem with this approach\n",
    "\n",
    "def tokenizer(doc):\n",
    "    return doc.split()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c287e8c5-6ec4-4f22-9974-27b5a9cf6975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 18)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n",
      "  (0, 8)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 4)\t2\n",
      "  (0, 5)\t1\n",
      "  (0, 0)\t1\n",
      "  (1, 7)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 14)\t1\n",
      "  (2, 9)\t1\n",
      "  (3, 16)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 3)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 12)\t1\n"
     ]
    }
   ],
   "source": [
    "# Bag of Words\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Inititalize the \"CountVectorize\" object,  Which is scikint learn\"s\n",
    "# bag of words tool\n",
    "bow_vec = CountVectorizer(token_pattern = None,\n",
    "                          tokenizer = tokenizer,\n",
    "                          ngram_range = (1,1),\n",
    "                          lowercase = False,\n",
    "                          preprocessor = None,\n",
    "                          stop_words = None)\n",
    "\n",
    "# fit_transform() does two function:\n",
    "# First, it fits and learns the vocabulary\n",
    "# second, it transform our trainning data into feature vectors\n",
    "# the input to fit_transform should be a list of strings\n",
    "\n",
    "dtm = bow_vec.fit_transform(df['text'])\n",
    "\n",
    "print(dtm.shape)\n",
    "print(type(dtm))\n",
    "print(dtm)\n",
    "# sắp xếp theo thứ tự điển \n",
    "# hiểu đơn giản là 0 8 là hàng 0 có từ vị trí 8 xuất hiện 1 lần"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2ceb153-30c6-4d59-b740-53beba7e9516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18\n",
      "\n",
      "Let's look at the vocabulary stored in the object: \n",
      "{'We': 8, 'are': 10, 'Learning': 4, 'Machine': 5, '$': 0, 'Processing': 7, 'natural': 16, '-': 1, 'language': 13, 'data.': 11, '10': 2, 'machine': 15, 'learning': 14, 'algorithms.': 9, 'we': 17, 'Are': 3, 'Mimicing': 6, 'intelligence': 12}\n",
      "\n",
      "Output Feature Names: ['$' '-' '10' 'Are' 'Learning' 'Machine' 'Mimicing' 'Processing' 'We'\n",
      " 'algorithms.' 'are' 'data.' 'intelligence' 'language' 'learning'\n",
      " 'machine' 'natural' 'we']\n"
     ]
    }
   ],
   "source": [
    "# we can look at unique words by using 'vocabulary_'\n",
    "\n",
    "print(f\"Vocabulary size: {len(bow_vec.vocabulary_)}\")\n",
    "print()\n",
    "print(f\"Let's look at the vocabulary stored in the object: \\n{bow_vec.vocabulary_}\")\n",
    "print()\n",
    "print(\"Output Feature Names:\", bow_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "525df666-d3d9-4e58-846e-d720a1268e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 2 1 0 0 1 0 1 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 1 0 0 0 1 0 1 0 0 1 0]\n",
      " [0 1 1 0 0 0 0 0 0 1 0 0 0 0 1 1 0 0]\n",
      " [0 0 0 1 0 0 1 0 0 0 0 0 1 0 0 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# sử dụng toarray để chuyển dữ liệu thành ma trận\n",
    "# phù hợp mô hình học ámy\n",
    "print(dtm.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "64c97bf7-dd30-4e53-b532-722dfe56330c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$</th>\n",
       "      <th>-</th>\n",
       "      <th>10</th>\n",
       "      <th>Are</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Machine</th>\n",
       "      <th>Mimicing</th>\n",
       "      <th>Processing</th>\n",
       "      <th>We</th>\n",
       "      <th>algorithms.</th>\n",
       "      <th>are</th>\n",
       "      <th>data.</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   $  -  10  Are  Learning  Machine  Mimicing  Processing  We  algorithms.  \\\n",
       "0  1  0   0    0         2        1         0           0   1            0   \n",
       "1  0  1   0    0         0        0         0           1   0            0   \n",
       "2  0  1   1    0         0        0         0           0   0            1   \n",
       "3  0  0   0    1         0        0         1           0   0            0   \n",
       "\n",
       "   are  data.  intelligence  language  learning  machine  natural  we  \n",
       "0    1      0             0         0         0        0        0   0  \n",
       "1    0      1             0         1         0        0        1   0  \n",
       "2    0      0             0         0         1        1        0   0  \n",
       "3    0      0             1         0         0        0        1   1  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the sparse matrix to a dataFrame\n",
    "\n",
    "pd.DataFrame(dtm.toarray(),\n",
    "             columns = bow_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc14aef-70a3-49e0-abbb-a07e85850956",
   "metadata": {},
   "source": [
    "# b. BoW Text Vectorization: Apply Countvectorizer with ngram_range = (1,2 ) and lowercase = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79c743c6-f600-4c59-91db-3d7f5610f90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 35)\n",
      "<class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "bow_vect = CountVectorizer(token_pattern=None,\n",
    "                           tokenizer=tokenizer,\n",
    "                           ngram_range=(1, 2), \n",
    "                           lowercase=False, \n",
    "                           preprocessor=None, \n",
    "                           stop_words=None)\n",
    "\n",
    "dtm = bow_vect.fit_transform(df['text'])\n",
    "\n",
    "print(dtm.shape)\n",
    "print(type(dtm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "888b94dd-7906-4375-9c2f-66db21503f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 17)\t1\n",
      "  (0, 20)\t1\n",
      "  (0, 8)\t2\n",
      "  (0, 11)\t1\n",
      "  (0, 0)\t1\n",
      "  (0, 18)\t1\n",
      "  (0, 21)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 9)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 30)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 24)\t1\n",
      "  (1, 22)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 31)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 25)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 28)\t1\n",
      "  (2, 26)\t1\n",
      "  (2, 19)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 29)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 27)\t1\n",
      "  (3, 30)\t1\n",
      "  (3, 33)\t1\n",
      "  (3, 6)\t1\n",
      "  (3, 13)\t1\n",
      "  (3, 23)\t1\n",
      "  (3, 34)\t1\n",
      "  (3, 7)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 32)\t1\n"
     ]
    }
   ],
   "source": [
    "print(dtm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e07ca58e-53bb-4634-bf9e-da17acff96ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "{'We': 17, 'are': 20, 'Learning': 8, 'Machine': 11, '$': 0, 'We are': 18, 'are Learning': 21, 'Learning Machine': 10, 'Machine Learning': 12, 'Learning $': 9, 'Processing': 15, 'natural': 30, '-': 1, 'language': 24, 'data.': 22, 'Processing natural': 16, 'natural -': 31, '- language': 2, 'language data.': 25, '10': 4, 'machine': 28, 'learning': 26, 'algorithms.': 19, '10 machine': 5, 'machine -': 29, '- learning': 3, 'learning algorithms.': 27, 'we': 33, 'Are': 6, 'Mimicing': 13, 'intelligence': 23, 'we Are': 34, 'Are Mimicing': 7, 'Mimicing natural': 14, 'natural intelligence': 32}\n"
     ]
    }
   ],
   "source": [
    "# We can loolk at unique words by using 'vocabulary_'\n",
    "print(len(bow_vect.vocabulary_))\n",
    "\n",
    "print(bow_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2d2d7db0-3658-4a3e-8cde-910d9ef0de2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Feature Names: ['$' '-' '- language' '- learning' '10' '10 machine' 'Are' 'Are Mimicing'\n",
      " 'Learning' 'Learning $' 'Learning Machine' 'Machine' 'Machine Learning'\n",
      " 'Mimicing' 'Mimicing natural' 'Processing' 'Processing natural' 'We'\n",
      " 'We are' 'algorithms.' 'are' 'are Learning' 'data.' 'intelligence'\n",
      " 'language' 'language data.' 'learning' 'learning algorithms.' 'machine'\n",
      " 'machine -' 'natural' 'natural -' 'natural intelligence' 'we' 'we Are']\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Feature Names:\", bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b0f81f18-4cae-47e2-b083-8e03c7db5576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 2 1 1 1 1 0 0 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 1 0 1 1 0 0 0 0 1 1 0 0 0]\n",
      " [0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to numpy array\n",
    "print(dtm.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "310e7041-502b-4044-beb3-90d275c2339d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>$</th>\n",
       "      <th>-</th>\n",
       "      <th>- language</th>\n",
       "      <th>- learning</th>\n",
       "      <th>10</th>\n",
       "      <th>10 machine</th>\n",
       "      <th>Are</th>\n",
       "      <th>Are Mimicing</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Learning $</th>\n",
       "      <th>...</th>\n",
       "      <th>language data.</th>\n",
       "      <th>learning</th>\n",
       "      <th>learning algorithms.</th>\n",
       "      <th>machine</th>\n",
       "      <th>machine -</th>\n",
       "      <th>natural</th>\n",
       "      <th>natural -</th>\n",
       "      <th>natural intelligence</th>\n",
       "      <th>we</th>\n",
       "      <th>we Are</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   $  -  - language  - learning  10  10 machine  Are  Are Mimicing  Learning  \\\n",
       "0  1  0           0           0   0           0    0             0         2   \n",
       "1  0  1           1           0   0           0    0             0         0   \n",
       "2  0  1           0           1   1           1    0             0         0   \n",
       "3  0  0           0           0   0           0    1             1         0   \n",
       "\n",
       "   Learning $  ...  language data.  learning  learning algorithms.  machine  \\\n",
       "0           1  ...               0         0                     0        0   \n",
       "1           0  ...               1         0                     0        0   \n",
       "2           0  ...               0         1                     1        1   \n",
       "3           0  ...               0         0                     0        0   \n",
       "\n",
       "   machine -  natural  natural -  natural intelligence  we  we Are  \n",
       "0          0        0          0                     0   0       0  \n",
       "1          0        1          1                     0   0       0  \n",
       "2          1        0          0                     0   0       0  \n",
       "3          0        1          0                     1   1       1  \n",
       "\n",
       "[4 rows x 35 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting the sparse matrix to a dataframe\n",
    "\n",
    "pd.DataFrame(dtm.toarray(), \n",
    "             columns=bow_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c85b93-695b-465a-9d85-d5d3f3641a47",
   "metadata": {},
   "source": [
    "# Term Frequency - Inverse Document Frequency ( TF_IDF)\n",
    "## khác biệt so với BoW\n",
    "###TF-IDF giải quyết nhược điểm lớn nhất của BoW: BoW coi tất cả các từ đều có tầm quan trọng như nhau. TF-IDF thì không. Nó đánh giá tầm quan trọng của từ, giúp các thuật toán học máy tập trung vào các từ khóa có ý nghĩa hơn, từ đó cải thiện hiệu suất của mô hình.\n",
    "\n",
    "1. Term Frequency\n",
    "-  measures how Frequently a term (word) appears in a documents()\n",
    "\n",
    "2. Inverse Document Frequency\n",
    "- Measures how important a term is within the entire corpus\n",
    "- It decreases the weight of terms that appear in many documents and increases the weight of terms that appear in fewer documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562037f9-8a8c-45df-be88-e6040680a8d2",
   "metadata": {},
   "source": [
    "## Advantage\n",
    "1. If the word is rare in the corpus, it will be given more importance\n",
    "2. If the word is more frequent in a document, it will be giive more importtance\n",
    "## disvantages\n",
    "same as BoW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9a30a-3a63-4a60-85e1-da86a5c3e4e9",
   "metadata": {},
   "source": [
    "# TF IDF Text Vectorization: Apply TfidfVVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "83760cf7-333c-4569-8483-1c82560adbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of output (# of docs, # of unique vocabulary): (4, 16)\n",
      "Type of output (i.e. Compressed Sparse Row (CSR) format): <class 'scipy.sparse._csr.csr_matrix'>\n"
     ]
    }
   ],
   "source": [
    "# TF_IDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf_vect = TfidfVectorizer(ngram_range = (1, 1), \n",
    "                             lowercase = False\n",
    "                            , preprocessor=None,\n",
    "                             stop_words=None, \n",
    "                            )\n",
    "# không cần sử dụng token_pattern vì nó amwcj định là r\"(?u)\\b\\w+\\b\"\n",
    "\n",
    "out = tfidf_vect.fit_transform(df['text'])\n",
    "print(f\"Shape of output (# of docs, # of unique vocabulary): {out.shape}\")\n",
    "\n",
    "print(f\"Type of output (i.e. Compressed Sparse Row (CSR) format): {type(out)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9f90c306-7066-4efe-b50e-9c8b9031a816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "{'We': 6, 'are': 8, 'Learning': 2, 'Machine': 3, 'Processing': 5, 'natural': 14, 'language': 11, 'data': 9, '10': 0, 'machine': 13, 'learning': 12, 'algorithms': 7, 'we': 15, 'Are': 1, 'Mimicing': 4, 'intelligence': 10}\n"
     ]
    }
   ],
   "source": [
    "print(len(tfidf_vect.vocabulary_))\n",
    "print(tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cf60976d-eac4-49bb-985e-79a1f68764d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10' 'Are' 'Learning' 'Machine' 'Mimicing' 'Processing' 'We' 'algorithms'\n",
      " 'are' 'data' 'intelligence' 'language' 'learning' 'machine' 'natural'\n",
      " 'we']\n"
     ]
    }
   ],
   "source": [
    "# nó sẽ khác nhau ở am trận, điểm của ma trận không chỉ dựa trên tần suất\n",
    "# từ trong văn bản đó mà còn dựa trên mức độ hiếm của từ đó trong toàn bộ tập dữ liệu\n",
    "print(tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7a4649cd-b756-4b93-bf4f-c54b98dbd0b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.         0.75592895 0.37796447 0.         0.\n",
      "  0.37796447 0.         0.37796447 0.         0.         0.\n",
      "  0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.         0.         0.52547275\n",
      "  0.         0.         0.         0.52547275 0.         0.52547275\n",
      "  0.         0.         0.41428875 0.        ]\n",
      " [0.5        0.         0.         0.         0.         0.\n",
      "  0.         0.5        0.         0.         0.         0.\n",
      "  0.5        0.5        0.         0.        ]\n",
      " [0.         0.46516193 0.         0.         0.46516193 0.\n",
      "  0.         0.         0.         0.         0.46516193 0.\n",
      "  0.         0.         0.36673901 0.46516193]]\n"
     ]
    }
   ],
   "source": [
    "# convert sparse matrix to nparray\n",
    "\n",
    "print(out.toarray())\n",
    "\n",
    "# ô nào càng lớn mức độ hiếm càng cao"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "272c620c-aad5-44f5-9416-447ec2872ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10</th>\n",
       "      <th>Are</th>\n",
       "      <th>Learning</th>\n",
       "      <th>Machine</th>\n",
       "      <th>Mimicing</th>\n",
       "      <th>Processing</th>\n",
       "      <th>We</th>\n",
       "      <th>algorithms</th>\n",
       "      <th>are</th>\n",
       "      <th>data</th>\n",
       "      <th>intelligence</th>\n",
       "      <th>language</th>\n",
       "      <th>learning</th>\n",
       "      <th>machine</th>\n",
       "      <th>natural</th>\n",
       "      <th>we</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.755929</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.377964</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.525473</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.414289</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.465162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.366739</td>\n",
       "      <td>0.465162</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    10       Are  Learning   Machine  Mimicing  Processing        We  \\\n",
       "0  0.0  0.000000  0.755929  0.377964  0.000000    0.000000  0.377964   \n",
       "1  0.0  0.000000  0.000000  0.000000  0.000000    0.525473  0.000000   \n",
       "2  0.5  0.000000  0.000000  0.000000  0.000000    0.000000  0.000000   \n",
       "3  0.0  0.465162  0.000000  0.000000  0.465162    0.000000  0.000000   \n",
       "\n",
       "   algorithms       are      data  intelligence  language  learning  machine  \\\n",
       "0         0.0  0.377964  0.000000      0.000000  0.000000       0.0      0.0   \n",
       "1         0.0  0.000000  0.525473      0.000000  0.525473       0.0      0.0   \n",
       "2         0.5  0.000000  0.000000      0.000000  0.000000       0.5      0.5   \n",
       "3         0.0  0.000000  0.000000      0.465162  0.000000       0.0      0.0   \n",
       "\n",
       "    natural        we  \n",
       "0  0.000000  0.000000  \n",
       "1  0.414289  0.000000  \n",
       "2  0.000000  0.000000  \n",
       "3  0.366739  0.465162  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Converting the sparse matrix to a dataframe\n",
    "\n",
    "pd.DataFrame(out.toarray(), \n",
    "             columns=tfidf_vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1b0b53-8774-4bff-8f0c-a1382860682b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
